\documentclass{article}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[section]{placeins}
\usepackage{longtable}
\begin{document}
\title{Smartcab \\ Udacity Machine Learning Nanodegree}
\author{Chiel Peters}
\maketitle

\section*{Observe what you see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?}

When the enforce deadline parameter is set to false the smartcab does eventually reach its destination. However it does so very slowly without any learning. Other interesting observations I made: the next waypoint is not always the most optimal route to take as it does not take into account the current state of the stop light/traffic however it is a good indicator for direction, the grid does not have boundaries that is falling off on the left returns the car on the right. There is no way to ignore a red if the action forward is chosen and the stoplight is red then the car will just stay in place.

\section*{What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?}

In order to model the smartcab and the environment three pieces of information are needed: stoplight status, traffic situation and route to destination. The first two can be found in the inputs variable: left and oncoming. For the route to destination I used the next waypoint variable. This gives a good indication of direction for the smartcab. The variable right in inputs is not used as it does not include any information relevant for the allowed moves of the smartcab. Also the deadline is not used in the state as the smartcab should always perform the optimal move and not perform any shortcuts to make the deadline. Another disadvantage of the deadline that the state space becomes very big as the deadline has a lot of possibilities (+-50) which slows down learning a lot. 
To summarize the state space consists of four variables [(State of stoplight),(Oncoming Traffic), (Left Traffic), (next waypoint)]. I believe these states sufficiently describe the problem as they convey information on all aspects of the smartcab and the environment. It should reach the destination by means of the next waypoint and learn the rules of the road by the stoplight and traffic variables. 

\section*{How many states in total exist for the smartcab in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?}

In total there are 2 (green,red) x 4 (None, left, right , forward) x 4 (None, left, right , forward) x 4 (None, left, right , forward) = 64 states. The None state is actually never reached for the current implementation of the next waypoint so in reality only 2 x 4 x 4 x 3 = 48 states are present, however for completeness and generality the other states are also included. It seems reasonable to assume that all 48 states are reached a number of times when the number of trials is high. Although the number of states is small the probability of each state is not uniformly distributed. With the current setup with two other cars on the grid there is only a very small probability of having both incoming traffic from the front and left at the same time. Therefore the Q-learning will have difficulties learning in these states as they do not occur very often however I believe that with a high number of trials the algorithm can also learn in these states.

\section*{What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?}

In the beginning smartcab is still acting random. This is because the Q-learning algorithm has not yet figured out the optimal move in every state. After a few trials however the agents behavior starts to resemble that of a optimized driver. It stops when the stoplight is red and the correct direction is not right and it follows the directions given by the next waypoint. This behavior is occuring because of the Q-matrix. In the beginning for every action in every state the Q-matrix will be 0. Therefore for a state the smartcab still acts random as there is no optimal action with a high utility. After a while it has explored a number actions for a number of states and the algorithm starts learning from the utilities, that are formed by the (future) rewards, which actions are optimal in which states. For a state the smartcab observes the past utilities of all actions in that state and is able to pick the action with the highest utility. To avoid local optima and to allow exploration the algorithm does sometimes still act random, this occurs epsilon percent of the time. After a large number of trials every state and action have been explored and the Q-matrix has learned the optimal action, that is the one with the highest utility, for every state.

\section*{Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?}

The succes rate is defined as the number of times the smartcab reaches its destination within the deadline. Although we could deploy 'smarter' metrics such as reward-per-action or actions over distance between start and end point in this case we prefer simplicity. As reaching the destination is not solely our goal the average number of penalties (either wrong direction driving/collisions or trying to run a red). There are three parameters which can be tuned: gamma (discount factor for future rewards), epsilon (exploration) and alpha (learning rate). The algorithm is run for 3 different values for each parameter (27 times in total). The values are based upon the values found in the lectures and online resources. The succes rate for each run is stated in the tables below together with the average penalty rate between brackets. The optimal configuration is $\gamma$ = .9, $\alpha$ = .03 and $\epsilon$ = .01 with a succes rate of 100 \%. However when we also consider the average number of penalties the configuration $\gamma$ = .95, $\alpha$ = .03 and $\epsilon$ = .01 might be better with a succes rate of 99\% but  only an average number of penalties per ride of .28.

\begin{table}
\centering
\begin{tabular}{c | c | c | c |}
\hline
$\epsilon$/$\alpha$ & .01 & .03 & .1 \\ \hline
.01 & 99 \% (.32)& 97 \% (.7)& 94 \% (2.4)\\ \hline
.03 & 99 \% (.82)& 98 \% (1.29)& 92 \% (4.1)\\ \hline
.05 & 96 \% (.82)& 95 \% (3.2)& 93 \% (3.81)\\ \hline
\end{tabular}
\caption{Success rates  (and average number of penalties per ride) for gamma ($\gamma$) equals .85}
\end{table}

\begin{table}
\centering
\begin{tabular}{c | c | c | c |}
\hline
$\epsilon$/$\alpha$ & .01 & .03 & .1 \\ \hline
.01 & 98 \% (.45)& 100 \% (.81)& 97 \% (.52)\\ \hline
.03 & 96 \% (1.8)& 96 \% (3.1)& 96 \% (2.5)\\ \hline
.05 & 97 \% (2.4)& 96 \% (3.3)&  92 \%  (2.7)\\ \hline
\end{tabular}
\caption{Success rates (and average number of penalties per ride) for gamma ($\gamma$) equals .9}
\end{table}

\begin{table}
\centering
\begin{tabular}{c | c | c | c |}
\hline
$\epsilon$/$\alpha$ & .01 & .03 & .1 \\ \hline
.01 & 99 \% (.38)& 99 \% (.28)& 96 \% (2.3)\\ \hline
.03 & 95 \% (2.68)& 98 \% (1.2)& 96 \% (1.1)\\ \hline
.05 & 95 \% (2.2)& 91 \% (3.1)& 97 \% (2.3)\\  \hline
\end{tabular}
\caption{Success rates (and average number of penalties per ride) for gamma ($\gamma$) equals .95}
\end{table}

\section*{Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?}

The agent will not find the optimal policy 'reach the destination in the minimum possible time and not incur any penalties'. One reason for this is that the smartcab follows the waypoints untill it reaches the destination. However the waypoints do not account for the current traffic and stoplight status. It therefore sometimes misses that it could take a right turn on a red signal to get closer to the destination and advises the smartcab to go forward which it cannot do because of the red light.

However the smartcab does find another optimal policy, because of the way the state space and learning are defined. Note that this only holds for the proportion of time when the smartcab performs exploitation on what is has learning instead of performing a random action (exploration). In the optimal policy it will follow the waypoints towards the destination while not violating any traffic lights or collisions. 

However it takes a long time to learn this. The Q-matrix is printed below for 100 trials. All the rows where there is traffic from the front (Oncoming is not None) and traffic from the left (Left is not None) are 0 since the smartcab has not observed traffic from both sides at the same time.For the other states it can be observed that in case of no traffic (Oncoming and Left are None) the smartcab has learned to follow the waypoint when the stoplight is green and it turns right when the stoplight is red and the waypoint is right. For cases with traffic from one side the smartcab has not learned enough and still does suboptimal actions. 

\begin{longtable}{llllrrrr}
\toprule
      &      &      &      &       NaN &      left &     right &   forward \\
light & Oncoming & Left & Waypoint &           &           &           &           \\
\midrule
\endhead
\midrule
\multicolumn{3}{r}{{Continued on next page}} \\
\midrule
\endfoot

\bottomrule
\endlastfoot
green & None & None & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.096299 &  4.965131 &  0.000000 & -0.029550 \\
      &      &      & right &  0.000000 & -0.029550 &  4.544914 & -0.015000 \\
      &      &      & forward &  0.000000 & -0.010022 & -0.015000 &  5.689972 \\
      &      & left & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 & -0.015000 &  0.143704 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.166448 &  0.000000 &  0.000000 \\
      &      & right & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.121732 \\
      &      & forward & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 & -0.007940 &  0.060000 \\
      & left & None & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.109252 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.363521 &  0.000000 &  0.000000 \\
      &      & left & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & right & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & forward & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      & right & None & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 & -0.030000 &  0.000000 &  0.360000 \\
      &      & left & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & right & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & forward & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      & forward & None & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.016260 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & left & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & right & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & forward & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
red & None & None & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 & -0.030000 & -0.011631 & -0.030000 \\
      &      &      & right &  0.000000 & -0.030000 &  4.722891 & -0.030000 \\
      &      &      & forward &  0.000000 & -0.030000 & -0.020299 & -0.087327 \\
      &      & left & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & right & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 & -0.030000 &  0.095430 &  0.000000 \\
      &      & forward & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 & -0.030000 & -0.030000 & -0.030000 \\
      & left & None & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 & -0.030000 & -0.015000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.278322 & -0.030000 \\
      &      &      & forward &  0.000000 & -0.030000 & -0.015000 & -0.030000 \\
      &      & left & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & right & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & forward & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      & right & None & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 & -0.030000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & left & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & right & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & forward & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      & forward & None & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 & -0.030000 & -0.015000 & -0.030000 \\
      &      & left & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & right & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      & forward & None &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & left &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & right &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
      &      &      & forward &  0.000000 &  0.000000 &  0.000000 &  0.000000 \\
\caption{Q-matrix found for optimal parameter set configuration}
\end{longtable}

\end{document}